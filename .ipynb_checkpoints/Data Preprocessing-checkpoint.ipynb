{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Phil/anaconda/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import arff # https://pypi.python.org/pypi/liac-arff\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "contractionsFile = open(\"english-contractions-list.txt\", \"r\")\n",
    "contractions = []\n",
    "for line in contractionsFile:\n",
    "    contractions = line.split(',')\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#object that holds a lot of our data\n",
    "\n",
    "class NewsObject:\n",
    "    id = '0'\n",
    "    targetTitle = []\n",
    "    targetDescription = \"\"\n",
    "    targetKeywords = []\n",
    "    targetParagraphs = []\n",
    "    targetCaptions = []\n",
    "    postText = []\n",
    "    postMedia = []\n",
    "    postTimestamp = ''\n",
    "    #truthMedian = ''\n",
    "    #truthMean = ''\n",
    "    #truthMode = ''\n",
    "    truthClass = \"\"\n",
    "    #truthJudgments = []\n",
    "    attributes = ()\n",
    "    \n",
    "    def __init__(self, line):\n",
    "        \n",
    "        self.id = line['id']\n",
    "        self.targetTitle= line['targetTitle']\n",
    "        self.targetKeywords = line['targetKeywords']\n",
    "        self.targetParagraphs = line['targetParagraphs']\n",
    "        self.targetCaptions = line['targetCaptions']\n",
    "        self.postText = line['postText']\n",
    "        self.postMedia = line['postMedia']\n",
    "        self.postTimestamp = line['postTimestamp']\n",
    "        \n",
    "    def addTruth(self, line):\n",
    "        #self.truthMedian = line['truthMedian']\n",
    "        #self.truthMean = line['truthMean']\n",
    "        #self.truthMode = line['truthMode']\n",
    "        if line['truthClass'] == 'clickbait':\n",
    "            self.truthClass = '1'\n",
    "        else:\n",
    "            self.truthClass = '0'\n",
    "        #self.truthJudgments = line['truthJudgments']\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import files\n",
    "instances = []\n",
    "\n",
    "with open('dataset/instances_train.jsonl') as file:\n",
    "    for line in file:\n",
    "        temp = NewsObject(json.loads(line))\n",
    "        instances.append(temp)\n",
    "        \n",
    "with open('dataset/truth_train.jsonl') as file2:\n",
    "    i = 0\n",
    "    for line in file2:\n",
    "        instances[i].addTruth(json.loads(line))\n",
    "        i += 1\n",
    "#print(instances[0].targetKeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(newsObject):\n",
    "    feat = {\n",
    "        \"wordCount\": 0, \"informal\": False, \"beginsQuestion\": False, \"beginsNum\": False,\n",
    "        \"beginsThis\": False, \"titleStopPerc\": 0, \"titleProperPerc\": 0, \"posSent\": 0,\n",
    "        \"neuSent\": 0, \"negSent\": 0, \"compoundSent\": 0, \"articleWords\": 0, \"titlePercVerbs\": 0,\n",
    "        \"unigrams\": 0, \"bigrams\": 0, \"trigrams\": 0, \"fourgrams\": 0, \"fivegrams\": 0, \"percNouns\": 0,\"unigramsArticle\": 0,\n",
    "        \"bigramsArticle\": 0, \"trigramsArticle\": 0, \"fourgramsArticle\": 0, \"fivegramsArticle\": 0, \"percAdj\": 0,\n",
    "        \"percAdv\": 0,  \"percentKeywordsInTitle\": 0, \"sentDiffTitleBody\": 0, \n",
    "        \"hasContractions\": False, \"has!\" : False, \"has?\": False, \"hasQuote\": False, \n",
    "        \"bodyPercProper\": 0, \"bodyPercAdv\": 0, \"bodyQuoteNum\": 0,\n",
    "        \"label\": newsObject.truthClass\n",
    "    }\n",
    "    \n",
    "    words = newsObject.postText[0].split(' ')\n",
    "    keywords = newsObject.targetKeywords\n",
    "    text = nltk.word_tokenize(newsObject.postText[0])\n",
    "    tokenizedList = nltk.pos_tag(text)\n",
    "    \n",
    "    \n",
    "    feat['wordCount'] = len(words)\n",
    "    \n",
    "    if words[0].isdigit():\n",
    "        feat['beginsNum'] = True\n",
    "    elif words[0].lower() == 'this':\n",
    "        feat['beginsThis'] = True\n",
    "    \n",
    "    numSame = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in keywords:\n",
    "            numSame += 1\n",
    "        if word in contractions:\n",
    "            feat['hasContractions'] = True\n",
    "  \n",
    "    numProper = 0\n",
    "    numStop = 0\n",
    "    numVerb = 0\n",
    "    numNoun = 0\n",
    "    numAdj = 0\n",
    "    numAdv = 0\n",
    "    \n",
    "    if len(tokenizedList) > 0:\n",
    "        firstWord = tokenizedList[0]\n",
    "        if firstWord[1] == 'MD' or firstWord[1] == 'WRB':\n",
    "            feat['beginsQuestion'] = True\n",
    "    \n",
    "    for partOfSpeech in tokenizedList:\n",
    "        if partOfSpeech[1] == 'NNP':\n",
    "            numProper += 1\n",
    "        elif partOfSpeech[1] == 'VB' or partOfSpeech[1] == 'VBP' or partOfSpeech[1] == 'VBD' or partOfSpeech[1] == 'VBN':\n",
    "            numVerb += 1\n",
    "        elif partOfSpeech[1] == 'NN':\n",
    "            numNoun += 1\n",
    "        elif partOfSpeech[1] == 'PRP':\n",
    "            f2 = True\n",
    "        elif partOfSpeech[1] == 'JJ':\n",
    "            numAdj += 1\n",
    "        elif partOfSpeech[1] == 'RB':\n",
    "            numAdv += 1\n",
    "        elif partOfSpeech[1] == '.':\n",
    "            if partOfSpeech[0] == '?':\n",
    "                feat['has?'] = True\n",
    "            elif partOfSpeech[0] == '!':\n",
    "                feat['has!'] = True\n",
    "        elif partOfSpeech[1] == \"''\" or partOfSpeech[1] == '\"\"':\n",
    "            feat['hasQuote'] = True\n",
    "        if(partOfSpeech[0] in stopWords):\n",
    "            numStop += 1\n",
    "\n",
    "    feat['titleStopPerc'] = round(numStop/feat['wordCount'], 2)\n",
    "    feat['titleProperPerc'] = round(numProper/feat['wordCount'], 2)\n",
    "    feat['titlePercVerbs'] = round(numVerb/feat['wordCount'], 2)\n",
    "    feat['percNouns'] = round(numNoun/feat['wordCount'], 2)\n",
    "    feat['percAdj'] = round(numAdj/feat['wordCount'], 2)\n",
    "    feat['percAdv'] = round(numAdv/feat['wordCount'], 2)\n",
    "    feat['percentKeywordsInTitle'] = round(numSame/feat['wordCount'], 2)\n",
    "    \n",
    "    feat['unigrams'] = sum(Counter(ngrams(text,1)).values())\n",
    "    feat['bigrams'] = sum(Counter(ngrams(text,2)).values())\n",
    "    feat['trigrams'] = sum(Counter(ngrams(text,3)).values())\n",
    "    feat['fourgrams'] = sum(Counter(ngrams(text,4)).values())\n",
    "    feat['fivegrams'] = sum(Counter(ngrams(text,5)).values())\n",
    "    \n",
    "    sentence = newsObject.postText[0]\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    feat['posSent'] = ss['pos']\n",
    "    feat['neuSent'] = ss['neu']\n",
    "    feat['negSent'] = ss['neg']\n",
    "    feat['compoundSent'] = ss['compound']\n",
    "    \n",
    "    paraSent = 0\n",
    "    countPara = 0\n",
    "    articleNumProp = 0\n",
    "    articleNumAdv = 0\n",
    "    ss2 = {}\n",
    "    for item in newsObject.targetParagraphs:\n",
    "        feat['articleWords'] += len(item.split())\n",
    "        text = nltk.word_tokenize(item)\n",
    "        tokenizedList = nltk.pos_tag(text)\n",
    "        feat['unigramsArticle'] += sum(Counter(ngrams(text,1)).values())\n",
    "        feat['bigramsArticle'] += sum(Counter(ngrams(text,2)).values())\n",
    "        feat['trigramsArticle'] += sum(Counter(ngrams(text,3)).values())\n",
    "        feat['fourgramsArticle'] += sum(Counter(ngrams(text,4)).values())\n",
    "        feat['fivegramsArticle'] += sum(Counter(ngrams(text,5)).values())\n",
    "        ss2 = sid.polarity_scores(item)\n",
    "        countPara += 1\n",
    "        paraSent += ss2['compound']\n",
    "        for word in tokenizedList:\n",
    "            if word[1] == 'NNP':\n",
    "                articleNumProp += 1\n",
    "            elif word[1] == 'RB':\n",
    "                articleNumAdv += 1\n",
    "            elif word[1] == \"''\" or word[1] == '\"\"':\n",
    "                feat['bodyQuoteNum'] += 1\n",
    "    \n",
    "    if feat['articleWords'] > 0:\n",
    "        feat['bodyPercProper'] = round(articleNumProp/feat['articleWords'], 2)\n",
    "        feat['bodyPercAdv'] = round(articleNumAdv/feat['articleWords'], 2)\n",
    "    \n",
    "    if countPara > 0:\n",
    "        paraSent = paraSent/countPara\n",
    "    \n",
    "    feat['sentDiffTitleBody'] = abs(feat['compoundSent'] - paraSent)\n",
    "    \n",
    "    \n",
    "    #do noun, adverb, adjective, proper noun analysis for text bodies\n",
    "        \n",
    "    featTuple = (feat['wordCount'], feat['informal'], feat['beginsQuestion'], feat['beginsNum'], \n",
    "            feat['beginsThis'], feat['titleStopPerc'], feat['titleProperPerc'], \n",
    "            feat['posSent'], feat['neuSent'], feat['negSent'], feat['compoundSent'], feat['articleWords'], \n",
    "            feat['titlePercVerbs'], feat['unigrams'], feat['bigrams'], feat['trigrams'], feat['fourgrams'], \n",
    "            feat['fivegrams'], feat['percNouns'], feat['unigramsArticle'],\n",
    "            feat['bigramsArticle'], feat['trigramsArticle'], feat['fourgramsArticle'], feat['fivegramsArticle'], \n",
    "            feat['percAdj'], feat['percAdv'], feat['percentKeywordsInTitle'], \n",
    "            feat['sentDiffTitleBody'], feat['hasContractions'], feat['has!'], feat['has?'], feat['hasQuote'],\n",
    "            feat['bodyPercProper'], feat['bodyPercAdv'], feat['bodyQuoteNum'], feat['label'])\n",
    "    newsObject.attributes = featTuple\n",
    "    return newsObject\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b00403d17b0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-902c5b3c21a4>\u001b[0m in \u001b[0;36mextractFeatures\u001b[0;34m(newsObject)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mfeat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'articleWords'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mtokenizedList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mfeat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unigramsArticle'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mfeat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bigramsArticle'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Phil/anaconda/lib/python3.6/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \"\"\"\n\u001b[1;32m    133\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Phil/anaconda/lib/python3.6/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en-ptb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Phil/anaconda/lib/python3.6/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mprev2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Phil/anaconda/lib/python3.6/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Do a secondary alphabetic sort, for stability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for item in instances:\n",
    "    item = extractFeatures(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dump to arff\n",
    "features = [(\"word count\", 'NUMERIC'),\n",
    "            (\"contains informal pronouns\", ['True', 'False']),\n",
    "            (\"Begins w/ question word\", ['True', 'False']),\n",
    "            (\"Begins w/ number\", ['True', 'False']),\n",
    "            (\"Begins with 'this'\", ['True', 'False']),\n",
    "            (\"percent stop words\", 'NUMERIC'),\n",
    "            (\"Percent proper nouns\", 'NUMERIC'),\n",
    "            (\"Pos sent\", 'NUMERIC'),\n",
    "            (\"Neu sent\", 'NUMERIC'),\n",
    "            (\"Neg sent\", 'NUMERIC'),\n",
    "            (\"Compound sent\", 'NUMERIC'),\n",
    "            (\"Article Length\", 'NUMERIC'),\n",
    "            (\"Percent verbs\", 'NUMERIC'),\n",
    "            (\"Unigrams\", 'NUMERIC'),\n",
    "            (\"Bigrams\", 'NUMERIC'),\n",
    "            (\"Trigrams\", 'NUMERIC'),\n",
    "            (\"Fourgrams\", 'NUMERIC'),\n",
    "            (\"Fivegrams\", 'NUMERIC'),\n",
    "            (\"Percent nouns\", 'NUMERIC'),\n",
    "            (\"Unigrams article body\", 'NUMERIC'),\n",
    "            (\"Bigrams article body\", 'NUMERIC'),\n",
    "            (\"Trigrams article body\", 'NUMERIC'),\n",
    "            (\"Fourgrams article body\", 'NUMERIC'),\n",
    "            (\"Fivegrams article body\", 'NUMERIC'),\n",
    "            (\"Percent adj\", 'NUMERIC'),\n",
    "            (\"Percent adv\", 'NUMERIC'),\n",
    "            (\"Percent keywords in title\", 'NUMERIC'),\n",
    "            (\"Difference in sent body v. title\", 'NUMERIC'),\n",
    "            ('Has contractions', ['True', 'False']),\n",
    "            ('Has exclamation', ['True', 'False']),\n",
    "            (\"Has question\", ['True', 'False']),\n",
    "            ('Title has quote', ['True','False']),\n",
    "            (\"Body percent proper\", 'NUMERIC'),\n",
    "            (\"Body percent adv\", 'NUMERIC'),\n",
    "            (\"Body num quotes\", 'NUMERIC'),\n",
    "            (\"label\", ['0', '1'])]\n",
    "data = {}\n",
    "data.setdefault('attributes', features)\n",
    "data.setdefault('description', '')\n",
    "data.setdefault('relation', 'clickbait_sample')\n",
    "data.setdefault('data', [])\n",
    "for item in instances:\n",
    "    if item.attributes[28] == '1':\n",
    "        print(item.postText[0])\n",
    "        print(item.attributes)\n",
    "    data['data'].append(item.attributes)\n",
    "\n",
    "with open('sample_train.arff', 'w') as f:\n",
    "    f.write(arff.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = nltk.word_tokenize(\"'sloppy man'\")\n",
    "tokenizedList = nltk.pos_tag(text1)\n",
    "print(tokenizedList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
