{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Philz zee Kill\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import arff # https://pypi.python.org/pypi/liac-arff\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#object that holds a lot of our data\n",
    "\n",
    "class NewsObject:\n",
    "    id = '0'\n",
    "    targetTitle = []\n",
    "    targetDescription = \"\"\n",
    "    targetKeywords = []\n",
    "    targetParagraphs = []\n",
    "    targetCaptions = []\n",
    "    postText = []\n",
    "    postMedia = []\n",
    "    postTimestamp = ''\n",
    "    #truthMedian = ''\n",
    "    #truthMean = ''\n",
    "    #truthMode = ''\n",
    "    truthClass = \"\"\n",
    "    #truthJudgments = []\n",
    "    attributes = ()\n",
    "    \n",
    "    def __init__(self, line):\n",
    "        \n",
    "        self.id = line['id']\n",
    "        self.targetTitle= line['targetTitle']\n",
    "        self.targetKeywords = line['targetKeywords']\n",
    "        self.targetParagraphs = line['targetParagraphs']\n",
    "        self.targetCaptions = line['targetCaptions']\n",
    "        self.postText = line['postText']\n",
    "        self.postMedia = line['postMedia']\n",
    "        self.postTimestamp = line['postTimestamp']\n",
    "        \n",
    "    def addTruth(self, line):\n",
    "        #self.truthMedian = line['truthMedian']\n",
    "        #self.truthMean = line['truthMean']\n",
    "        #self.truthMode = line['truthMode']\n",
    "        if line['truthClass'] == 'clickbait':\n",
    "            self.truthClass = '1'\n",
    "        else:\n",
    "            self.truthClass = '0'\n",
    "        #self.truthJudgments = line['truthJudgments']\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import files\n",
    "instances = []\n",
    "\n",
    "with open('dataset/instances_train.jsonl') as file:\n",
    "    for line in file:\n",
    "        temp = NewsObject(json.loads(line))\n",
    "        instances.append(temp)\n",
    "        \n",
    "with open('dataset/truth_train.jsonl') as file2:\n",
    "    i = 0\n",
    "    for line in file2:\n",
    "        instances[i].addTruth(json.loads(line))\n",
    "        i += 1\n",
    "#print(instances[0].targetKeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(newsObject):\n",
    "    f1 = 0\n",
    "    f2 = False\n",
    "    f3 = False\n",
    "    f4 = False\n",
    "    f5 = False\n",
    "    f6 = 0\n",
    "    f7 = 0\n",
    "    f8 = 0\n",
    "    f9 = 0\n",
    "    f10 = 0\n",
    "    f11 = 0\n",
    "    f12 = 0\n",
    "    f13 = 0\n",
    "    f14 = 0\n",
    "    f15 = 0\n",
    "    f16 = 0\n",
    "    f17 = 0\n",
    "    f18 = 0\n",
    "    f21 = 0\n",
    "    f22 = 0\n",
    "    f23 = 0\n",
    "    f24 = 0\n",
    "    f25 = 0\n",
    "    f26 = 0\n",
    "    f27 = 0\n",
    "    f28 = 0\n",
    "    f29 = 0\n",
    "    f30 = 0\n",
    "    f100 = newsObject.truthClass\n",
    "\n",
    "    words = newsObject.postText[0].split(' ')\n",
    "    f1 = len(words)\n",
    "    keywords = newsObject.targetKeywords\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    \n",
    "    if words[0].isdigit():\n",
    "        f4 = True\n",
    "    elif words[0].lower() == 'this':\n",
    "        f5 = True\n",
    "    \n",
    "    numSame = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in keywords:\n",
    "            numSame += 1\n",
    "            \n",
    "        \n",
    "    \n",
    "    text = nltk.word_tokenize(newsObject.postText[0])\n",
    "    tokenizedList = nltk.pos_tag(text)\n",
    "\n",
    "    numProper = 0\n",
    "    numStop = 0\n",
    "    numVerb = 0\n",
    "    numNoun = 0\n",
    "    numAdj = 0\n",
    "    numAdv = 0\n",
    "    \n",
    "    if len(tokenizedList) > 0:\n",
    "        firstWord = tokenizedList[0]\n",
    "        if firstWord[1] == 'MD' or firstWord[1] == 'WRB':\n",
    "            f3 = True\n",
    "    \n",
    "    for partOfSpeech in tokenizedList:\n",
    "        if partOfSpeech[1] == 'NNP':\n",
    "            numProper += 1\n",
    "        elif partOfSpeech[1] == 'VB' or partOfSpeech[1] == 'VBP' or partOfSpeech[1] == 'VBD' or partOfSpeech[1] == 'VBN':\n",
    "            numVerb += 1\n",
    "        elif partOfSpeech[1] == 'NN':\n",
    "            numNoun += 1\n",
    "        elif partOfSpeech[1] == 'PRP':\n",
    "            f2 = True\n",
    "        elif partOfSpeech[1] == 'ADJ':\n",
    "            numAdj += 1\n",
    "        elif partOfSpeech[1] == 'RB':\n",
    "            numAdv += 1\n",
    "        if(partOfSpeech[0] in stopWords):\n",
    "            numStop += 1\n",
    "\n",
    "    f7 = round(numStop/f1, 2)\n",
    "    f8 = round(numProper/f1, 2)\n",
    "    f13 = round(numVerb/f1, 2)\n",
    "    f21 = round(numNoun/f1, 2)\n",
    "    f27 = round(numAdj/f1, 2)\n",
    "    f28 = round(numAdv/f1, 2)\n",
    "    f29 = round(numSame/f1, 2)\n",
    "    \n",
    "    f14 = sum(Counter(ngrams(text,1)).values())\n",
    "    f15 = sum(Counter(ngrams(text,2)).values())\n",
    "    f16 = sum(Counter(ngrams(text,3)).values())\n",
    "    f17 = sum(Counter(ngrams(text,4)).values())\n",
    "    f18 = sum(Counter(ngrams(text,5)).values())\n",
    "    \n",
    "    sentence = newsObject.postText[0]\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    f6 = ss['pos']\n",
    "    f9 = ss['neu']\n",
    "    f10 = ss['neg']\n",
    "    f11 = ss['compound']\n",
    "    \n",
    "    paraSent = 0\n",
    "    countPara = 0\n",
    "    ss2 = 0\n",
    "    for item in newsObject.targetParagraphs:\n",
    "        f12 += len(item.split())\n",
    "        text = nltk.word_tokenize(item)\n",
    "        f22 += sum(Counter(ngrams(text,1)).values())\n",
    "        f23 += sum(Counter(ngrams(text,2)).values())\n",
    "        f24 += sum(Counter(ngrams(text,3)).values())\n",
    "        f25 += sum(Counter(ngrams(text,4)).values())\n",
    "        f26 += sum(Counter(ngrams(text,5)).values())\n",
    "        ss2 = sid.polarity_scores(item)\n",
    "        countPara += 1\n",
    "        paraSent += ss2['compound']\n",
    "        \n",
    "    if countPara > 0:\n",
    "        paraSent = paraSent/countPara\n",
    "    \n",
    "    f30 = abs(f11 - paraSent)\n",
    "        \n",
    "    feat = (f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15, \n",
    "            f16, f17, f18, f21, f22, f23, f24, f25, f26, f27, f28, f29, f30, f100)\n",
    "    newsObject.attributes = feat\n",
    "    return newsObject\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in instances:\n",
    "    item = extractFeatures(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dump to arff\n",
    "features = [(\"word count\", 'NUMERIC'),\n",
    "            (\"contains informal pronouns\", ['True', 'False']),\n",
    "            (\"Begins w/ question word\", ['True', 'False']),\n",
    "            (\"Begins w/ number\", ['True', 'False']),\n",
    "            (\"Begins with 'this'\", ['True', 'False']),\n",
    "            (\"percent stop words\", 'NUMERIC'),\n",
    "            (\"Percent proper nouns\", 'NUMERIC'),\n",
    "            (\"Pos sent\", 'NUMERIC'),\n",
    "            (\"Neu sent\", 'NUMERIC'),\n",
    "            (\"Neg sent\", 'NUMERIC'),\n",
    "            (\"Compound sent\", 'NUMERIC'),\n",
    "            (\"Article Length\", 'NUMERIC'),\n",
    "            (\"Percent verbs\", 'NUMERIC'),\n",
    "            (\"Unigrams\", 'NUMERIC'),\n",
    "            (\"Bigrams\", 'NUMERIC'),\n",
    "            (\"Trigrams\", 'NUMERIC'),\n",
    "            (\"Fourgrams\", 'NUMERIC'),\n",
    "            (\"Fivegrams\", 'NUMERIC'),\n",
    "            (\"Percent nouns\", 'NUMERIC'),\n",
    "            (\"Unigrams article body\", 'NUMERIC'),\n",
    "            (\"Bigrams article body\", 'NUMERIC'),\n",
    "            (\"Trigrams article body\", 'NUMERIC'),\n",
    "            (\"Fourgrams article body\", 'NUMERIC'),\n",
    "            (\"Fivegrams article body\", 'NUMERIC'),\n",
    "            (\"Percent adj\", 'NUMERIC'),\n",
    "            (\"Percent adv\", 'NUMERIC'),\n",
    "            (\"Percent keywords in title\", 'NUMERIC'),\n",
    "            (\"Difference in sent body v. title\", 'NUMERIC'),\n",
    "            (\"label\", ['0', '1'])]\n",
    "data = {}\n",
    "data.setdefault('attributes', features)\n",
    "data.setdefault('description', '')\n",
    "data.setdefault('relation', 'clickbait_sample')\n",
    "data.setdefault('data', [])\n",
    "for item in instances:\n",
    "    if item.attributes[24] == '1':\n",
    "        print(item.postText[0])\n",
    "        print(item.attributes)\n",
    "    data['data'].append(item.attributes)\n",
    "\n",
    "with open('sample_train.arff', 'w') as f:\n",
    "    f.write(arff.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('metal', 'NN')]\n",
      "dict_values([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "text1 = nltk.word_tokenize(\"metal\")\n",
    "tokenizedList = nltk.pos_tag(text1)\n",
    "print(tokenizedList)\n",
    "text = nltk.word_tokenize(instances[100].postText[0])\n",
    "bigrams = ngrams(text,1)\n",
    "print(Counter(bigrams).values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
