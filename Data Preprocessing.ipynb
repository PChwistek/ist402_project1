{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Phil/anaconda/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import arff # https://pypi.python.org/pypi/liac-arff\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "contractionsFile = open(\"english-contractions-list.txt\", \"r\")\n",
    "contractions = []\n",
    "for line in contractionsFile:\n",
    "    contractions = line.split(',')\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#object that holds a lot of our data\n",
    "\n",
    "class NewsObject:\n",
    "    id = '0'\n",
    "    targetTitle = []\n",
    "    targetDescription = \"\"\n",
    "    targetKeywords = []\n",
    "    targetParagraphs = []\n",
    "    targetCaptions = []\n",
    "    postText = []\n",
    "    postMedia = []\n",
    "    postTimestamp = ''\n",
    "    #truthMedian = ''\n",
    "    #truthMean = ''\n",
    "    #truthMode = ''\n",
    "    truthClass = \"\"\n",
    "    #truthJudgments = []\n",
    "    attributes = ()\n",
    "    \n",
    "    def __init__(self, line):\n",
    "        \n",
    "        self.id = line['id']\n",
    "        self.targetTitle= line['targetTitle']\n",
    "        self.targetKeywords = line['targetKeywords']\n",
    "        self.targetParagraphs = line['targetParagraphs']\n",
    "        self.targetCaptions = line['targetCaptions']\n",
    "        self.postText = line['postText']\n",
    "        self.postMedia = line['postMedia']\n",
    "        self.postTimestamp = line['postTimestamp']\n",
    "        \n",
    "    def addTruth(self, line):\n",
    "        #self.truthMedian = line['truthMedian']\n",
    "        #self.truthMean = line['truthMean']\n",
    "        #self.truthMode = line['truthMode']\n",
    "        if line['truthClass'] == 'clickbait':\n",
    "            self.truthClass = '1'\n",
    "        else:\n",
    "            self.truthClass = '0'\n",
    "        #self.truthJudgments = line['truthJudgments']\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import files\n",
    "instances = []\n",
    "\n",
    "with open('dataset/instances_train.jsonl') as file:\n",
    "    for line in file:\n",
    "        temp = NewsObject(json.loads(line))\n",
    "        instances.append(temp)\n",
    "        \n",
    "with open('dataset/truth_train.jsonl') as file2:\n",
    "    i = 0\n",
    "    for line in file2:\n",
    "        instances[i].addTruth(json.loads(line))\n",
    "        i += 1\n",
    "#print(instances[0].targetKeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractFeatures(newsObject):\n",
    "    feat = {\n",
    "        \"wordCount\": 0, \"informal\": False, \"beginsQuestion\": False, \"beginsNum\": False,\n",
    "        \"beginsThis\": False, \"titleStopPerc\": 0, \"titleProperPerc\": 0, \"posSent\": 0,\n",
    "        \"neuSent\": 0, \"negSent\": 0, \"compoundSent\": 0, \"articleWords\": 0, \"titlePercVerbs\": 0,\n",
    "        \"unigrams\": 0, \"bigrams\": 0, \"trigrams\": 0, \"fourgrams\": 0, \"fivegrams\": 0, \"percNouns\": 0,\"unigramsArticle\": 0,\n",
    "        \"bigramsArticle\": 0, \"trigramsArticle\": 0, \"fourgramsArticle\": 0, \"fivegramsArticle\": 0, \"percAdj\": 0,\n",
    "        \"percAdv\": 0,  \"percentKeywordsInTitle\": 0, \"sentDiffTitleBody\": 0, \"hasContractions\": False, \n",
    "        \"label\": newsObject.truthClass\n",
    "    }\n",
    "    \n",
    "    words = newsObject.postText[0].split(' ')\n",
    "    keywords = newsObject.targetKeywords\n",
    "    text = nltk.word_tokenize(newsObject.postText[0])\n",
    "    tokenizedList = nltk.pos_tag(text)\n",
    "    \n",
    "    \n",
    "    feat['wordCount'] = len(words)\n",
    "    \n",
    "    if words[0].isdigit():\n",
    "        feat['beginsNum'] = True\n",
    "    elif words[0].lower() == 'this':\n",
    "        feat['beginsThis'] = True\n",
    "    \n",
    "    numSame = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in keywords:\n",
    "            numSame += 1\n",
    "        if word in contractions:\n",
    "            feat['hasContractions'] = True\n",
    "  \n",
    "    numProper = 0\n",
    "    numStop = 0\n",
    "    numVerb = 0\n",
    "    numNoun = 0\n",
    "    numAdj = 0\n",
    "    numAdv = 0\n",
    "    \n",
    "    if len(tokenizedList) > 0:\n",
    "        firstWord = tokenizedList[0]\n",
    "        if firstWord[1] == 'MD' or firstWord[1] == 'WRB':\n",
    "            feat['beginsQuestion'] = True\n",
    "    \n",
    "    for partOfSpeech in tokenizedList:\n",
    "        if partOfSpeech[1] == 'NNP':\n",
    "            numProper += 1\n",
    "        elif partOfSpeech[1] == 'VB' or partOfSpeech[1] == 'VBP' or partOfSpeech[1] == 'VBD' or partOfSpeech[1] == 'VBN':\n",
    "            numVerb += 1\n",
    "        elif partOfSpeech[1] == 'NN':\n",
    "            numNoun += 1\n",
    "        elif partOfSpeech[1] == 'PRP':\n",
    "            f2 = True\n",
    "        elif partOfSpeech[1] == 'DJ':\n",
    "            numAdj += 1\n",
    "        elif partOfSpeech[1] == 'RB':\n",
    "            numAdv += 1\n",
    "        if(partOfSpeech[0] in stopWords):\n",
    "            numStop += 1\n",
    "\n",
    "    feat['titleStopPerc'] = round(numStop/feat['wordCount'], 2)\n",
    "    feat['titleProperPerc'] = round(numProper/feat['wordCount'], 2)\n",
    "    feat['titlePercVerbs'] = round(numVerb/feat['wordCount'], 2)\n",
    "    feat['percNouns'] = round(numNoun/feat['wordCount'], 2)\n",
    "    feat['percAdj'] = round(numAdj/feat['wordCount'], 2)\n",
    "    feat['percAdv'] = round(numAdv/feat['wordCount'], 2)\n",
    "    feat['percentKeywordsInTitle'] = round(numSame/feat['wordCount'], 2)\n",
    "    \n",
    "    feat['unigrams'] = sum(Counter(ngrams(text,1)).values())\n",
    "    feat['bigrams'] = sum(Counter(ngrams(text,2)).values())\n",
    "    feat['trigrams'] = sum(Counter(ngrams(text,3)).values())\n",
    "    feat['fourgrams'] = sum(Counter(ngrams(text,4)).values())\n",
    "    feat['fivegrams'] = sum(Counter(ngrams(text,5)).values())\n",
    "    \n",
    "    sentence = newsObject.postText[0]\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    feat['posSent'] = ss['pos']\n",
    "    feat['neuSent'] = ss['neu']\n",
    "    feat['negSent'] = ss['neg']\n",
    "    feat['compoundSent'] = ss['compound']\n",
    "    \n",
    "    paraSent = 0\n",
    "    countPara = 0\n",
    "    ss2 = {}\n",
    "    for item in newsObject.targetParagraphs:\n",
    "        feat['articleWords'] += len(item.split())\n",
    "        text = nltk.word_tokenize(item)\n",
    "        feat['unigramsArticle'] += sum(Counter(ngrams(text,1)).values())\n",
    "        feat['bigramsArticle'] += sum(Counter(ngrams(text,2)).values())\n",
    "        feat['trigramsArticle'] += sum(Counter(ngrams(text,3)).values())\n",
    "        feat['fourgramsArticle'] += sum(Counter(ngrams(text,4)).values())\n",
    "        feat['fivegramsArticle'] += sum(Counter(ngrams(text,5)).values())\n",
    "        ss2 = sid.polarity_scores(item)\n",
    "        countPara += 1\n",
    "        paraSent += ss2['compound']\n",
    "        \n",
    "    if countPara > 0:\n",
    "        paraSent = paraSent/countPara\n",
    "    \n",
    "    feat['sentDiffTitleBody'] = abs(feat['compoundSent'] - paraSent)\n",
    "    \n",
    "    \n",
    "    #do noun, adverb, adjective, proper noun analysis for text bodies\n",
    "        \n",
    "    featTuple = (feat['wordCount'], feat['informal'], feat['beginsQuestion'], feat['beginsNum'], \n",
    "            feat['beginsThis'], feat['titleStopPerc'], feat['titleProperPerc'], \n",
    "            feat['posSent'], feat['neuSent'], feat['negSent'], feat['compoundSent'], feat['articleWords'], \n",
    "            feat['titlePercVerbs'], feat['unigrams'], feat['bigrams'], feat['trigrams'], feat['fourgrams'], \n",
    "            feat['fivegrams'], feat['percNouns'], feat['unigramsArticle'],\n",
    "            feat['bigramsArticle'], feat['trigramsArticle'], feat['fourgramsArticle'], feat['fivegramsArticle'], \n",
    "            feat['percAdj'], feat['percAdv'], feat['percentKeywordsInTitle'], \n",
    "            feat['sentDiffTitleBody'], feat['hasContractions'], feat['label'])\n",
    "    newsObject.attributes = featTuple\n",
    "    return newsObject\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in instances:\n",
    "    item = extractFeatures(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dump to arff\n",
    "features = [(\"word count\", 'NUMERIC'),\n",
    "            (\"contains informal pronouns\", ['True', 'False']),\n",
    "            (\"Begins w/ question word\", ['True', 'False']),\n",
    "            (\"Begins w/ number\", ['True', 'False']),\n",
    "            (\"Begins with 'this'\", ['True', 'False']),\n",
    "            (\"percent stop words\", 'NUMERIC'),\n",
    "            (\"Percent proper nouns\", 'NUMERIC'),\n",
    "            (\"Pos sent\", 'NUMERIC'),\n",
    "            (\"Neu sent\", 'NUMERIC'),\n",
    "            (\"Neg sent\", 'NUMERIC'),\n",
    "            (\"Compound sent\", 'NUMERIC'),\n",
    "            (\"Article Length\", 'NUMERIC'),\n",
    "            (\"Percent verbs\", 'NUMERIC'),\n",
    "            (\"Unigrams\", 'NUMERIC'),\n",
    "            (\"Bigrams\", 'NUMERIC'),\n",
    "            (\"Trigrams\", 'NUMERIC'),\n",
    "            (\"Fourgrams\", 'NUMERIC'),\n",
    "            (\"Fivegrams\", 'NUMERIC'),\n",
    "            (\"Percent nouns\", 'NUMERIC'),\n",
    "            (\"Unigrams article body\", 'NUMERIC'),\n",
    "            (\"Bigrams article body\", 'NUMERIC'),\n",
    "            (\"Trigrams article body\", 'NUMERIC'),\n",
    "            (\"Fourgrams article body\", 'NUMERIC'),\n",
    "            (\"Fivegrams article body\", 'NUMERIC'),\n",
    "            (\"Percent adj\", 'NUMERIC'),\n",
    "            (\"Percent adv\", 'NUMERIC'),\n",
    "            (\"Percent keywords in title\", 'NUMERIC'),\n",
    "            (\"Difference in sent body v. title\", 'NUMERIC'),\n",
    "            ('Has contractions', ['True', 'False']),\n",
    "            (\"label\", ['0', '1'])]\n",
    "data = {}\n",
    "data.setdefault('attributes', features)\n",
    "data.setdefault('description', '')\n",
    "data.setdefault('relation', 'clickbait_sample')\n",
    "data.setdefault('data', [])\n",
    "for item in instances:\n",
    "    if item.attributes[28] == '1':\n",
    "        print(item.postText[0])\n",
    "        print(item.attributes)\n",
    "    data['data'].append(item.attributes)\n",
    "\n",
    "with open('sample_train.arff', 'w') as f:\n",
    "    f.write(arff.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = nltk.word_tokenize(\"metal\")\n",
    "tokenizedList = nltk.pos_tag(text1)\n",
    "print(tokenizedList)\n",
    "text = nltk.word_tokenize(instances[100].postText[0])\n",
    "bigrams = ngrams(text,1)\n",
    "print(Counter(bigrams).values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
