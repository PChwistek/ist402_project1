{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.twitter import Query, Streamer, Twitter, TweetViewer, TweetWriter, credsfromfile\n",
    "import json\n",
    "import arff # https://pypi.python.org/pypi/liac-arff\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import word_tokenize, TweetTokenizer\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "contractionsFile = open(\"english-contractions-list.txt\", \"r\")\n",
    "contractions = []\n",
    "for line in contractionsFile:\n",
    "    contractions = line.split(',')\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "tknz = TweetTokenizer()\n",
    "oauth = {\"app_key\":\"\", \n",
    "           \"app_secret\":\"\", \n",
    "           \"oauth_token\":\"\", \n",
    "           \"oauth_token_secret\":\"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#object that holds a lot of our data\n",
    "\n",
    "class NewsObject:\n",
    "    id = '0'\n",
    "    targetTitle = []\n",
    "    targetDescription = \"\"\n",
    "    targetKeywords = []\n",
    "    targetParagraphs = []\n",
    "    targetCaptions = []\n",
    "    postText = []\n",
    "    postMedia = []\n",
    "    postTimestamp = ''\n",
    "    #truthMedian = ''\n",
    "    #truthMean = ''\n",
    "    #truthMode = ''\n",
    "    truthClass = \"0\"\n",
    "    #truthJudgments = []\n",
    "    attributes = ()\n",
    "    \n",
    "    def __init__(self, line):\n",
    "        \n",
    "        self.id = line['id']\n",
    "        self.targetTitle= line['targetTitle']\n",
    "        self.targetKeywords = line['targetKeywords']\n",
    "        self.targetParagraphs = line['targetParagraphs']\n",
    "        self.targetCaptions = line['targetCaptions']\n",
    "        self.postText = line['postText']\n",
    "        self.postMedia = line['postMedia']\n",
    "        self.postTimestamp = line['postTimestamp']\n",
    "        \n",
    "    def addTruth(self, line):\n",
    "        #self.truthMedian = line['truthMedian']\n",
    "        #self.truthMean = line['truthMean']\n",
    "        #self.truthMode = line['truthMode']\n",
    "        if line['truthClass'] == 'clickbait':\n",
    "            self.truthClass = '1'\n",
    "        else:\n",
    "            self.truthClass = '0'\n",
    "        #self.truthJudgments = line['truthJudgments']\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import files\n",
    "instances = []\n",
    "\n",
    "with open('dataset/instances_test.jsonl') as file:\n",
    "    for line in file:\n",
    "        temp = NewsObject(json.loads(line))\n",
    "        instances.append(temp)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/truth_train.jsonl') as file2:\n",
    "    i = 0\n",
    "    for line in file2:\n",
    "        truth = json.loads(line)\n",
    "        instances[int(truth['id'])].addTruth(truth)\n",
    "#print(instances[0].targetKeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractFeatures(newsObject):\n",
    "    feat = {\n",
    "        \"wordCount\": 0, \"informal\": False, \"beginsQuestion\": False, \"beginsNum\": False,\n",
    "        \"beginsThis\": False, \"titleStopPerc\": 0, \"titleProperPerc\": 0, \"posSent\": 0,\n",
    "        \"neuSent\": 0, \"negSent\": 0, \"compoundSent\": 0, \"articleWords\": 0, \"titlePercVerbs\": 0,\n",
    "        \"unigrams\": 0, \"bigrams\": 0, \"trigrams\": 0, \"fourgrams\": 0, \"fivegrams\": 0, \"percNouns\": 0,\"unigramsArticle\": 0,\n",
    "        \"bigramsArticle\": 0, \"trigramsArticle\": 0, \"fourgramsArticle\": 0, \"fivegramsArticle\": 0, \"percAdj\": 0,\n",
    "        \"percAdv\": 0,  \"percentKeywordsInTitle\": 0, \"sentDiffTitleBody\": 0, \n",
    "        \"hasContractions\": False, \"has!\" : False, \"has?\": False, \"hasQuote\": False, \n",
    "        \"bodyPercProper\": 0, \"bodyPercAdv\": 0, \"bodyQuoteNum\": 0, \"lengthOfCaption\": 0, \"PosSentDifference\": 0,\n",
    "        \"NeuSentDifference\": 0, \"NegSentDifference\": 0, \"percPersonal\" : 0, \"percPersonalBody\": 0, \n",
    "        \"hasThat\": False, \"startsWithThat\": False, \"hasThis\": False, \"hasMedia\": False, \"isRetweet\": False, \n",
    "        \"hasMention\": False, \"hasHashtag\": False,\n",
    "        \"label\": newsObject.truthClass\n",
    "    }\n",
    "    \n",
    "    words = newsObject.postText[0].split(' ')\n",
    "    keywords =  [x.lower() for x in newsObject.targetKeywords]\n",
    "    text = nltk.word_tokenize(newsObject.postText[0])\n",
    "    tokenizedList = nltk.pos_tag(text)\n",
    "    twitterTokenized = tknz.tokenize(newsObject.postText[0])\n",
    "\n",
    "    \n",
    "    if len(newsObject.postMedia) > 0:\n",
    "        feat['hasMedia'] = True\n",
    "    \n",
    "    if len(twitterTokenized) > 0:\n",
    "        if twitterTokenized[0] == 'RT':\n",
    "            feat['isRetweet'] = True\n",
    "    \n",
    "    for tWord in twitterTokenized:\n",
    "        if tWord[0] == '@':\n",
    "            feat['hasMention'] = True\n",
    "        elif tWord[0] == '#':\n",
    "            feat['hasHashtag'] = True\n",
    "    \n",
    "    feat['wordCount'] = len(words)\n",
    "    \n",
    "    if words[0].isdigit():\n",
    "        feat['beginsNum'] = True\n",
    "    elif words[0].lower() == 'this':\n",
    "        feat['beginsThis'] = True\n",
    "        feat['hasThis'] = True\n",
    "    elif words[0].lower() == 'that':\n",
    "        feat['startsWithThat'] = True\n",
    "        feat['hasThat'] = True\n",
    "    \n",
    "    numSame = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word.lower() in keywords:\n",
    "            numSame += 1\n",
    "        if word.lower() in contractions:\n",
    "            feat['hasContractions'] = True\n",
    "        elif word.lower() == 'this':\n",
    "            feat['hasThis'] = True\n",
    "        elif word.lower() == 'that':\n",
    "            feat['hasThat'] = True\n",
    "  \n",
    "    numProper = 0\n",
    "    numStop = 0\n",
    "    numVerb = 0\n",
    "    numNoun = 0\n",
    "    numAdj = 0\n",
    "    numAdv = 0\n",
    "    numPersonal = 0\n",
    "    \n",
    "    if len(tokenizedList) > 0:\n",
    "        firstWord = tokenizedList[0]\n",
    "        if firstWord[1] == 'MD' or firstWord[1] == 'WRB':\n",
    "            feat['beginsQuestion'] = True\n",
    "    \n",
    "    for partOfSpeech in tokenizedList:\n",
    "        if partOfSpeech[1] == 'NNP':\n",
    "            numProper += 1\n",
    "        elif partOfSpeech[1] == 'VB' or partOfSpeech[1] == 'VBP' or partOfSpeech[1] == 'VBD' or partOfSpeech[1] == 'VBN':\n",
    "            numVerb += 1\n",
    "        elif partOfSpeech[1] == 'NN':\n",
    "            numNoun += 1\n",
    "        elif partOfSpeech[1] == 'PRP':\n",
    "            feat['informal'] = True\n",
    "            numPersonal += 1\n",
    "        elif partOfSpeech[1] == 'JJ':\n",
    "            numAdj += 1\n",
    "        elif partOfSpeech[1] == 'RB':\n",
    "            numAdv += 1\n",
    "        elif partOfSpeech[1] == '.':\n",
    "            if partOfSpeech[0] == '?':\n",
    "                feat['has?'] = True\n",
    "            elif partOfSpeech[0] == '!':\n",
    "                feat['has!'] = True\n",
    "        elif partOfSpeech[1] == \"''\" or partOfSpeech[1] == '\"\"':\n",
    "            feat['hasQuote'] = True\n",
    "        if(partOfSpeech[0].lower() in stopWords):\n",
    "            numStop += 1\n",
    "\n",
    "    feat['titleStopPerc'] = round(numStop/feat['wordCount'], 2)\n",
    "    feat['titleProperPerc'] = round(numProper/feat['wordCount'], 2)\n",
    "    feat['titlePercVerbs'] = round(numVerb/feat['wordCount'], 2)\n",
    "    feat['percPersonal'] = round(numPersonal/feat['wordCount'], 2)\n",
    "    feat['percNouns'] = round(numNoun/feat['wordCount'], 2)\n",
    "    feat['percAdj'] = round(numAdj/feat['wordCount'], 2)\n",
    "    feat['percAdv'] = round(numAdv/feat['wordCount'], 2)\n",
    "    feat['percentKeywordsInTitle'] = round(numSame/feat['wordCount'], 2)\n",
    "    \n",
    "    feat['unigrams'] = sum(Counter(ngrams(text,1)).values())\n",
    "    feat['bigrams'] = sum(Counter(ngrams(text,2)).values())\n",
    "    feat['trigrams'] = sum(Counter(ngrams(text,3)).values())\n",
    "    feat['fourgrams'] = sum(Counter(ngrams(text,4)).values())\n",
    "    feat['fivegrams'] = sum(Counter(ngrams(text,5)).values())\n",
    "    \n",
    "    sentence = newsObject.postText[0]\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    feat['posSent'] = ss['pos']\n",
    "    feat['neuSent'] = ss['neu']\n",
    "    feat['negSent'] = ss['neg']\n",
    "    feat['compoundSent'] = ss['compound']\n",
    "    \n",
    "    paraSent = 0\n",
    "    paraNegSent = 0\n",
    "    paraNeuSent = 0\n",
    "    paraPosSent = 0\n",
    "    countPara = 0\n",
    "    articleNumProp = 0\n",
    "    articleNumAdv = 0\n",
    "    numPersonalBody = 0 \n",
    "    ss2 = {}\n",
    "    for item in newsObject.targetParagraphs:\n",
    "        feat['articleWords'] += len(item.split())\n",
    "        text = nltk.word_tokenize(item)\n",
    "        tokenizedList = nltk.pos_tag(text)\n",
    "        feat['unigramsArticle'] += sum(Counter(ngrams(text,1)).values())\n",
    "        feat['bigramsArticle'] += sum(Counter(ngrams(text,2)).values())\n",
    "        feat['trigramsArticle'] += sum(Counter(ngrams(text,3)).values())\n",
    "        feat['fourgramsArticle'] += sum(Counter(ngrams(text,4)).values())\n",
    "        feat['fivegramsArticle'] += sum(Counter(ngrams(text,5)).values())\n",
    "        ss2 = sid.polarity_scores(item)\n",
    "        countPara += 1\n",
    "        paraSent += ss2['compound']\n",
    "        paraNegSent += ss2['neg']\n",
    "        paraNeuSent += ss2['neu']\n",
    "        paraPosSent += ss2['pos']\n",
    "        for word in tokenizedList:\n",
    "            if word[1] == 'NNP':\n",
    "                articleNumProp += 1\n",
    "            elif word[1] == 'RB':\n",
    "                articleNumAdv += 1\n",
    "            elif word[1] == 'PRP':\n",
    "                numPersonalBody += 1\n",
    "            elif word[1] == \"''\" or word[1] == '\"\"':\n",
    "                feat['bodyQuoteNum'] += 1\n",
    "    \n",
    "    if feat['articleWords'] > 0:\n",
    "        feat['bodyPercProper'] = round(articleNumProp/feat['articleWords'], 2)\n",
    "        feat['bodyPercAdv'] = round(articleNumAdv/feat['articleWords'], 2)\n",
    "        feat['percPersonalBody'] = round(numPersonalBody/feat['articleWords'], 2)\n",
    "    \n",
    "    if countPara > 0:\n",
    "        paraSent = paraSent/countPara\n",
    "        paraNegSent = paraNegSent/countPara\n",
    "        paraNeuSent = paraNeuSent/countPara\n",
    "        paraPosSent = paraPosSent/countPara\n",
    "    \n",
    "    feat['sentDiffTitleBody'] = abs(feat['compoundSent'] - paraSent)\n",
    "    feat['NegSentDifference'] = abs(feat['negSent'] - paraNegSent)\n",
    "    feat['NeuSentDifference'] = abs(feat['neuSent'] - paraNeuSent)\n",
    "    feat['PosSentDifference'] = abs(feat['posSent'] - paraPosSent)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    for item in newsObject.targetCaptions:\n",
    "        feat['lengthOfCaption'] += len(item)\n",
    "        \n",
    "        \n",
    "    featTuple = (feat['wordCount'], feat['informal'], feat['beginsQuestion'], feat['beginsNum'], \n",
    "            feat['beginsThis'], feat['titleStopPerc'], feat['titleProperPerc'], \n",
    "            feat['posSent'], feat['neuSent'], feat['negSent'], feat['compoundSent'], feat['articleWords'], \n",
    "            feat['titlePercVerbs'], feat['unigrams'], feat['bigrams'], feat['trigrams'], feat['fourgrams'], \n",
    "            feat['fivegrams'], feat['percNouns'], feat['unigramsArticle'],\n",
    "            feat['bigramsArticle'], feat['trigramsArticle'], feat['fourgramsArticle'], feat['fivegramsArticle'], \n",
    "            feat['percAdj'], feat['percAdv'], feat['percentKeywordsInTitle'], \n",
    "            feat['sentDiffTitleBody'], feat['hasContractions'], feat['has!'], feat['has?'], feat['hasQuote'],\n",
    "            feat['bodyPercProper'], feat['bodyPercAdv'], feat['bodyQuoteNum'], \n",
    "            feat['lengthOfCaption'], feat['PosSentDifference'], feat['NeuSentDifference'],\n",
    "            feat['NegSentDifference'],feat['percPersonal'], feat['percPersonalBody'],\n",
    "            feat['hasThat'], feat['startsWithThat'], feat['hasThis'], feat['hasMedia'], feat['isRetweet'],\n",
    "            feat['hasMention'], feat['hasHashtag'],\n",
    "            feat['label'])\n",
    "    newsObject.attributes = featTuple\n",
    "    return newsObject\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for item in instances:\n",
    "    item = extractFeatures(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dump to arff\n",
    "features = [(\"word count\", 'NUMERIC'),\n",
    "            (\"contains informal pronouns\", ['True', 'False']),\n",
    "            (\"Begins w/ question word\", ['True', 'False']),\n",
    "            (\"Begins w/ number\", ['True', 'False']),\n",
    "            (\"Begins with 'this'\", ['True', 'False']),\n",
    "            (\"percent stop words\", 'NUMERIC'),\n",
    "            (\"Percent proper nouns\", 'NUMERIC'),\n",
    "            (\"Pos sent\", 'NUMERIC'),\n",
    "            (\"Neu sent\", 'NUMERIC'),\n",
    "            (\"Neg sent\", 'NUMERIC'),\n",
    "            (\"Compound sent\", 'NUMERIC'),\n",
    "            (\"Article Length\", 'NUMERIC'),\n",
    "            (\"Percent verbs\", 'NUMERIC'),\n",
    "            (\"Unigrams\", 'NUMERIC'),\n",
    "            (\"Bigrams\", 'NUMERIC'),\n",
    "            (\"Trigrams\", 'NUMERIC'),\n",
    "            (\"Fourgrams\", 'NUMERIC'),\n",
    "            (\"Fivegrams\", 'NUMERIC'),\n",
    "            (\"Percent nouns\", 'NUMERIC'),\n",
    "            (\"Unigrams article body\", 'NUMERIC'),\n",
    "            (\"Bigrams article body\", 'NUMERIC'),\n",
    "            (\"Trigrams article body\", 'NUMERIC'),\n",
    "            (\"Fourgrams article body\", 'NUMERIC'),\n",
    "            (\"Fivegrams article body\", 'NUMERIC'),\n",
    "            (\"Percent adj\", 'NUMERIC'),\n",
    "            (\"Percent adv\", 'NUMERIC'),\n",
    "            (\"Percent keywords in title\", 'NUMERIC'),\n",
    "            (\"Difference in sent body v. title\", 'NUMERIC'),\n",
    "            ('Has contractions', ['True', 'False']),\n",
    "            ('Has exclamation', ['True', 'False']),\n",
    "            (\"Has question\", ['True', 'False']),\n",
    "            ('Title has quote', ['True','False']),\n",
    "            (\"Body percent proper\", 'NUMERIC'),\n",
    "            (\"Body percent adv\", 'NUMERIC'),\n",
    "            (\"Body num quotes\", 'NUMERIC'),\n",
    "            (\"Length Of Caption\", 'NUMERIC'),\n",
    "            (\"Pos Sent difference\", 'NUMERIC'),\n",
    "            (\"Neu sent difference\", 'NUMERIC'),\n",
    "            (\"Neg sent difference\", 'NUMERIC'),\n",
    "            (\"Percent personal pronouns\", 'NUMERIC'),\n",
    "            (\"Percent perosnal pronouns article\", 'NUMERIC'),\n",
    "            (\"Has that\", ['True', 'False']),\n",
    "            ('Starts with that', ['True', 'False']),\n",
    "            ('Has this', ['True', 'False']),\n",
    "            (\"Has media\", ['True', 'False']),\n",
    "            (\"Is Retweet\", ['True', 'False']),\n",
    "            (\"Has mention\", ['True', 'False']),\n",
    "            (\"Has Hashtag\", ['True', 'False']),\n",
    "            (\"label\", ['0', '1'])]\n",
    "data = {}\n",
    "data.setdefault('attributes', features)\n",
    "data.setdefault('description', '')\n",
    "data.setdefault('relation', 'clickbait_sample')\n",
    "data.setdefault('data', [])\n",
    "for item in instances:\n",
    "    if item.attributes[48] == '1':\n",
    "        print(item.postText[0])\n",
    "        print(item.attributes)\n",
    "    data['data'].append(item.attributes)\n",
    "\n",
    "with open('sample_train.arff', 'w') as f:\n",
    "    f.write(arff.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'twitter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2b874f23038b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStreamer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moauth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtwitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtwitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'#BlackLivesMatter'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'twitter' is not defined"
     ]
    }
   ],
   "source": [
    "client = Streamer(**oauth)\n",
    "results = twitter.cursor(twitter.search, q='#BlackLivesMatter')\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"apple\"\n",
    "str1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
